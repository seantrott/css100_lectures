{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9e23e1-5049-463e-9e9d-a43dc3731825",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Introduction to language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176c5597-ac8d-43f4-9bde-8742941e2133",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a077d-019a-40d5-81af-fa49acfcfc59",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Lecture plan\n",
    "\n",
    "- What is a \"language model\"?\n",
    "  - Brief overview of $n$-gram models.\n",
    "- The rise of \"neural\" language models (NLMs).\n",
    "  - Feedforward language models.\n",
    "  - The **transformer** architecture and self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05deea67-fb9e-43fd-b2ca-db1bba0ffcab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is a \"language model\"?\n",
    "\n",
    "> A **language model** assigns a *probability* to a sequence of words or characters, and can also be used to make predictions about a *given word* in a particular context.\n",
    "\n",
    "*I like my coffee with cream and ___*\n",
    "\n",
    "- Sugar?\n",
    "- Salt?\n",
    "- Mercury?\n",
    "\n",
    "These probabilities are *learned* looking at the statistics of actual language use on a large corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499585e3-24d4-4aba-bb99-60ac06dc02af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Key premises\n",
    "\n",
    "- **Language**: We're making predictions about *words* (or *characters*).\n",
    "- **Probability**: Some words (or characters) are more likely in some contexts than others.\n",
    "- **Order**: Language unfolds over time, i.e., *sequentially*.\n",
    "\n",
    "Thus, we want a system to make *probabilistic predictions* about a given word (or character) based on the preceding context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dbcccb-60dd-4c07-ae79-68e53be2c30c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### $n$-grams: a simple approach\n",
    "\n",
    "> An $n$-**gram** language model (LM) assigns probabilities to a word $w_t$ given the previous $n - 1$ words, based on the proportion of times $w_t$ occurs after those exact words.\n",
    "\n",
    "- A *unigram* model considers only $w_t$.\n",
    "- A *bigram* mdoel considers $w_t$ and the word before ($w_{t-1}$).\n",
    "- A *trigram model considers $w_t$ and the two words immediatedly before.\n",
    "\n",
    "And so on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f15fa5-d54e-41d3-966b-e870a14ea8ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Check-in\n",
    "\n",
    "Recall this example from earlier:\n",
    "\n",
    "> *I like my coffee with cream and ___*\n",
    "\n",
    "How would you calculate $p(sugar)$ using:\n",
    "\n",
    "- a unigram model?\n",
    "- a bigram model?\n",
    "- a trigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430ec0d4-a91a-46cc-9786-3b3aebe079e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ce9ee-9cd8-49dc-aeff-1836c7ea982c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Different amounts of context\n",
    "\n",
    "- A unigram model considers just the **base rate** of *sugar*, i.e., compared to size of entire corpus ($N$).\n",
    "\n",
    "$\\Large p(sugar) = \\frac{Count(sugar)}{N}$\n",
    "\n",
    "- A bigram model considers the relative probability of the phrase *and sugar*.\n",
    "\n",
    "$\\Large p(sugar | and) = \\frac{Count(and \\: sugar)}{Count(and)}$\n",
    "\n",
    "- A trigram model considers the relative probability of the phrase *cream and sugar*.\n",
    "\n",
    "$\\Large p(sugar | cream \\: and)= \\frac{Count(cream \\: and \\: sugar)}{Count(cream \\: and)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8facb-5716-41d1-9593-fe7379b32db0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Check-in\n",
    "\n",
    "How would you expect a unigram, bigram, and trigram model to perform differently for the different **completions**?\n",
    "\n",
    "- Sugar?\n",
    "- Salt?\n",
    "- Mercury?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174dd967-e3ce-4601-b2ed-4278b42f86ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72516ead-6099-4e13-864d-2375fb36a4d0",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The role of $n$\n",
    "\n",
    "- A larger value of $n$ means your predictions are **conditioned** on more context.\n",
    "- In general, a larger $n$ means more *accurate* predictions.\n",
    "   - Knowing more about what came before helps you know what comes next.\n",
    "- But a larger $n$ also leads to issues with **data sparsity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a502854-c949-4979-aebe-0bcb0ea9868d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Data sparsity and the problem of zeroes\n",
    "\n",
    "> When $n$ is too large, your data are **sparse**, i.e., the chance that you've seen *exactly* this sequence is just very low.\n",
    "\n",
    "- Practically, that means the probability of many sequences will be $0$!\n",
    "- This is very bad for the $n$-gram model.\n",
    "   - If $p(w) = 0$ for any $w$ in a sequence, then the joint probability of the entire sequence is also 0.\n",
    "\n",
    "**Check-in**: What might be a way to address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b5bfb-8e07-4a12-b300-01f9130df64b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Smoothing: a workaround\n",
    "\n",
    "> **Smoothing** refers to a variety of techniques designed to deal with data sparsity, i.e., by eliminating zeroes.\n",
    "\n",
    "- A common method is [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), where some *constant* is added to the count of each possible sequence.\n",
    "- This ensures no probability is zero.\n",
    "\n",
    "In general, these are somewhat clunky workarounds for a fundamental problem with $n$-gram models: they depend on the *exact tokens* in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4807c1-9be0-4856-a864-f87b0607c33d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The rise of \"neural\" LMs\n",
    "\n",
    "> A **neural language model** uses a neural network architecture for the language modeling task.\n",
    "\n",
    "Large neural language models like ChatGPT are called **large language models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490f90-2e09-4751-87e0-d673a1c05e25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Refresher: feedforward architecture\n",
    "\n",
    "- In a **feedforward architecture**, there are no *recurrent connections* between layers.\n",
    "- Each layer *transforms* input representations to produce *output* predictions.\n",
    "\n",
    "<img src=\"img/networks/ffn.png\" width=\"300\" alt=\"Simple FFN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f217d-1d89-4bfc-9261-42a19b474741",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Check-in\n",
    "\n",
    "Knowing what we know about word embeddings and $n$-gram models, how might you modify the feedforward architecture for the language modeling task?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9dfcc3d-0c2e-421c-a50f-1c2d975ab615",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Feed-forward neural language models\n",
    "\n",
    "> A **feedforward language model** uses a feed-forward neural network to assign probabilities to word $w_t$ using representations of previous words.\n",
    "\n",
    "\n",
    "<img src=\"img/llm/ffn_llm.png\" width=\"500\" alt=\"FFN for language modeling\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66030af4-ae52-4055-8eed-f7998849aa89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Features of feedforward language models\n",
    "\n",
    "- Like a $n$-gram model, relies on a *fixed context window*.\n",
    "- Unlike an $n$-gram model, uses **embeddings** for context words.\n",
    "    - Allows for better **generalizations** about *types of words*.\n",
    "- During training, errors are used as a signal to **update weights**.\n",
    "   - This helps the LM make better and better predictions.\n",
    "\n",
    "\n",
    "**Key limitation**: Not very good at incorporating context——how to figure out what's *relevant*?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f51598-6ac9-41df-94a4-5f7d44142cb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The advent of *attention*\n",
    "\n",
    "> **Attention** is a mechanism that (metaphorically) allows an LLM to “focus” (or “attend”) on specific elements in a sequence.\n",
    "\n",
    "[Interactive tutorial and visualization](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing)\n",
    "\n",
    "But how is this *calculated*?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54934161-b342-46fe-9a2a-93c1137e6a0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Dot-product attention: a simplification\n",
    "\n",
    "- The first \"attention\" mechanism was a simple **dot-product**.\n",
    "- Given two vectors, the dot product measures their similarity.\n",
    "\n",
    "$\\Large u \\cdot v$\n",
    "\n",
    "- Given a sequence of **word embeddings**, this yields a *matrix* reflecting the similarity of each pair of words.\n",
    "- These attention scores are then subject to a **softmax** function, turning them into a probability distribution.\n",
    "\n",
    "$\\Large softmax(x_i) = \\frac{e^{x_i}}{\\sum_je^{x_j}}$\n",
    "\n",
    "- These **attention scores** are then used to produce a *weighted average* of all the word embeddings for each word in the sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f350a32f-daea-49b4-8583-50f89fef2460",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Dot-product attention in action (pt. 1)\n",
    "\n",
    "- First, we initialize a matrix of embeddings (suppose these represent three different words).\n",
    "- Then, we get the pairwise dot products between each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11068451-3732-4dc2-b973-1fa495d47b4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0., 2.],\n",
       "        [0., 2., 2.],\n",
       "        [2., 2., 4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Embeddings\n",
    "E = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0],  # word 1\n",
    "    [0.0, 1.0, 0.0, 1.0],  # word 2\n",
    "    [1.0, 1.0, 1.0, 1.0],  # word 3\n",
    "])\n",
    "### Attention scores\n",
    "attention_scores = E @ E.T\n",
    "attention_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bfedb07-83bd-4513-a910-eecfc1eb9b72",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Dot-product attention in action (pt. 2)\n",
    "\n",
    "- Now, we *scale* our scores.\n",
    "- Then apply *softmax*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f358a1-f415-40ac-8923-f9004d6b3003",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [0., 1., 1.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Scale\n",
    "scale = np.sqrt(E.shape[1])  # sqrt of embedding dimension\n",
    "scaled_scores = attention_scores / scale\n",
    "scaled_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb6fb87-e442-4edf-8c3d-26dc5381a360",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4223, 0.1554, 0.4223],\n",
       "        [0.1554, 0.4223, 0.4223],\n",
       "        [0.2119, 0.2119, 0.5761]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get attention scores\n",
    "attention_probs = F.softmax(scaled_scores, dim=1)\n",
    "attention_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e50cc274-7d11-425b-a86a-04328c1b3035",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Interpreting the attention scores\n",
    "\n",
    "- Each *row* represents the attention between that particular word ($w_i$) and each other words.\n",
    "- The numbers represent the *probability distribution* over that word's attention.\n",
    "- A higher score means more *attention*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf871339-74de-46a1-8a35-8275de22bc71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4223, 0.1554, 0.4223])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Attention for word 1\n",
    "attention_probs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e15ad79d-29ce-4d3b-ae39-35714f949e08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Dot-product attention in action (pt. 3)\n",
    "\n",
    "- Finally, we create *new contextualized output vectors* $O_i$ for each word.\n",
    "- This is the *average* of all our words, weighted by the attention scores $\\alpha$.\n",
    "\n",
    "$O_i = \\sum_j \\alpha_{i, j}\\cdot E_j$\n",
    "\n",
    "- This results in a new *custom* vector for each word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "541f7f6d-bac0-4101-9abf-ee7f3b3bd4ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8446, 0.5777, 0.8446, 0.5777],\n",
       "        [0.5777, 0.8446, 0.5777, 0.8446],\n",
       "        [0.7881, 0.7881, 0.7881, 0.7881]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = attention_probs @ E  # shape: (3, 4)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82e37f42-9e14-4fb5-a0a9-3dcb9a983641",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### The limits of dot-product attention\n",
    "\n",
    "- Dot-product attention is very *coarse*: just considers *similarity* between original word embeddings.\n",
    "- But *relevance* might depend on all sorts of things.\n",
    "    - Syntactic relationship.\n",
    "    - Semantic relationship.\n",
    "    - Previous mentions.\n",
    "- Modern **transformer** language models use a more sophisticated mechanism for self-attention."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3772f1a1-1b67-4579-958e-ed701d705686",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Modern self-attention\n",
    "\n",
    "> In **self-attention**, the relevance of each word to each other is calculated in context and shared, informing the model’s predictions.\n",
    "\n",
    "- Similar to dot-product attention, but relies on *transformations* of each vector.\n",
    "- Three crucial **transformations**:\n",
    "    - *Query*: What is the current word \"looking for\"?\n",
    "    - *Key*: What is each other word \"offering\" to match that query?\n",
    "    - *Value*: What's the \"content\" of all the words?\n",
    "\n",
    "Kind of abstract, so let's look at a diagram.e "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eba32fd-9f14-4243-a5d2-11f74080d88a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### Attention, visualized\n",
    "\n",
    "- Schematic of **attention operations**.\n",
    "- Again: like dot-product operation, but now we're working with *queries* and *keys* instead of original vectors.\n",
    "\n",
    "<img src=\"img/llm/attention.png\" width=\"300\" alt=\"Graph of self attention.\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "deab759b-8860-4e2a-b7a6-2dfeff461c26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## The Transformer\n",
    "\n",
    "> The **transformer** is a *neural network architecture* that (now) is commonly used for LMs. A crucial feature is the **self-attention mechanism**.\n",
    "\n",
    "- Transformers rely on *self-attention*.\n",
    "- Usually **multi-headed**: multiple attention \"heads\" per layer.\n",
    "- Each layer consists of the *transformer block*. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b33286e-6103-4e0e-8d21-b31197216768",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The Transformer \"Block\"\n",
    "\n",
    "<img src=\"img/llm/transformer.png\" width=\"500\" alt=\"The crucial transformer block.\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2440de2c-7158-48b4-908f-5d1d306f0439",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Multi-headed attention\n",
    "\n",
    "- Multiple \"heads\" at each layer, can learn to track multiple things.\n",
    "- Oversimplified example: one head tracks syntax, another tracks meaning, etc...\n",
    "- In actuality, the **function** of heads is not programmed top-down.\n",
    "    - An entire [research field](https://www.neelnanda.io/mechanistic-interpretability/glossary) is dedicated to understanding these systems!\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26175cd3-f058-4654-88d1-6cfaee2cedc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Pre-trained models\n",
    "\n",
    "> A **pre-trained language model (PLM)** is a model trained on the next-token prediction task on a large corpus.\n",
    "\n",
    "- Researchers can download these models (e.g., from [HuggingFace](https://huggingface.co/)) and use them in some task.\n",
    "- PLMs can also be *fine-tuned* for downstream tasks.\n",
    "- Larger PLMs are surprisingly powerful: can do lots of things, despite only being trained to predict words.\n",
    "- Next lecture, we will work with an actual *pre-trained language model*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4061d822-f2d3-4e2b-8be4-0f7d2b3018b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "- A **language model** is simply a predictive model that assigns probabilities to word sequences.\n",
    "- Modern language models use the **transformer** architecture, which relies on a mechanism called *self-attention*.\n",
    "- These models are quite large and complicated, and there's still much we don't understand about how or why they work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
